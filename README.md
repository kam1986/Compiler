# Compiler
This is going to be a toy compiler for now with all stages implemented by my self.

When done it will be a general purpose concurrent reversible functional language. Reversible in the sense that if f is a function in program P then are the reverse function g also in P, hence f(g(x)) = g(f(x)) = x.

### OBS! See documentation below

## wokring
    * Lexer
    * Parser
    * Symbol Table Interface implemented, which ease code changes, and enable testing different scooping schemes depending on the symboltable 

## TODO
    * more testing on all parts
   
    * LR(k) for user defined k where the table is represented by a compressed table of an array of arrays.
    * rewriting campability of CFG to make the input grammar more easy to write
    * Implementaition of indentation sensitivity of the parser
   
    * Changing implementation of tokens and arguments in the lexer/parser to a two interface implementation for easier maintaince.
    
    * Type Checker
        - for the functional, imperative and intermidate language, based on SML type system.
        
    * Reversible constraint checker.

    * Optimizer (both in high and low level language)
    
    * Interpreter
    
    * Code generator which produce valid Rust code as output to be compiled, with focus on Rust bad compiler handling of tail recursion.
   
    
### when interpreter works
    * Intermediate language (2 kind, a typed and untyped)
    * Assembler (Arm is first focus)
    * Linker to avoid external software
    * Loader -||-


# Documentation

### disclaimers
As of now this is of version 0.0.1 (alpha) The Lexer is working, but is not optimised.
The parser will accept SLR Grammar only and it will make parse errors if you got some of the declared tokens missing in the productions when the token hare not defined as the last one. all tokens that should be filtered out afterward should be the last token in the list of tokens, since this will minimize the underlying DFA
If you try to decipher the type annotation of the input to the Lexer constructor you will get confused.
This will be handled later.

Both Lexer and Parser runs in linear time complexity, it can be used on unbuffered streams since it do no backtracking of the underlying input. 

## Lexer
The 'Lexer' is a DFA based lexer interpreter, which in general are the same as a generator with the minor detail that it does not produces a file to be compiled, but produces a data structure that can be used directly inside FSharp interactive. which makes test based implementation easier.

To define a lexer you give it an array of string representation of Regex patterns. followed by the user defined token type and optional conversion function.

```Fsharp
open Token
open Lexer


// use Enum or descreminated unions with no trayling data to define tokens
// using Hash() under the hood so any diviation to this will blow up the table size Extremely.
type TokenType = 
    | INT  | FLOAT
    | PLUS | MINUS | TIMES | DIVIDE 
    | LPAR | RPAR  | NOISE  // noise is intantionally last, since it then will be ommitted inside the DFA

// Define Lexing Scheme.
let lexer =
    [|
        "\+"                                := PLUS
        "\-"                                := MINUS
        "/"                                 := DIVIDE
        "\*"                                := TIMES
        "\("                                := LPAR
        "\)"                                := RPAR
        "[0-9]+"                            != int      --> INT
        "([0-9]+.[0-9]*)|([0-9]*.[0-9]+)"   != float    --> FLOAT
       
        // noise
        " *"                                := NOISE
        "\n"                                := NOISE
        "\r"                                := NOISE
        "\t"                                := NOISE
    |]
    |> Lexer

```
In the code snip above, we see some clever use of infix operators to make the patterns readable.
there are two types of pattersn

```Fsharp
Regex := Token
Regex != func --> Token
```
The first define a keyword token, since no data should be kept other than position and type we ommit transformation function
The second define values to be used in the language. := transform the left and right side into the proper type.
R != F --> T combined take the value found under regex R, transform it into the proper typed value with function F and mark it as token type T. OBS! The function F will panic if it is unable to parse the collected token data.

To generate the Tokens we parse the lexer scheme generated by Lexer to the Tokens function with the content

```Fsharp
    let content = FromString " 1 + 2 - 2 * 3"
    Tokens lexer content                               // return a sequence of tokens from the language of the 'lexer'
    |> Seq.filter (fun token -> TypeOf token <> NOISE) // user defined filter of tokens
 ```
 
The Tokens function transform the input string into a sequence of tokens, this means that the lexer will only find a token when ask by the parser. Since it returns a token sequence, we can filter it from any unwanted tokens, this is purposely made user definable since it makes it more verbose and more versatile. the Token type has 3 functions which takes a token.

```Fsharp
TypeOf: 't Token -> 't 
ValueOf: 't Token -> 'value
PosOf: 't Token -> Position
```
These are used in parsing of the tokens, they are this explicit named to make code readable. TypeOf return the instant type of the token, this is generally used as the user for filtering purposes and testing. ValueOf unwrap the value collected from the string i.e. a number if INT, a float if FLOAT.. and so furth.

## Parser

This is also a interpreter rather than an generator. As of version 0.0.1 it is only a simple left right parser.
As with the lexer we define a type to explain the CFG patterns.

The parser constructor takes user defined productions, and transforms it into a Stack based state machine.

```Fsharp
open Productions
open Parser

type e = Exp | Exp' | Val

// OBS! if for some reason you havn't implemented all Tokens into the productions, it will course an error at compiletime.
let parser =
    Productions [
        Exp => [
            [%Exp; !PLUS; %Exp'] 
            >> fun args -> ValueOf args.[0] + ValueOf args.[2]

            [%Exp; !MINUS; %Exp'] 
            >> fun args -> ValueOf args.[0] - ValueOf args.[2]

            [%Exp']
            >> fun args -> ValueOf args.[0]
        ]

        Exp' => [
            [%Exp'; !TIMES; %Val] 
            >> fun args -> ValueOf args.[0] * ValueOf args.[2]

            [%Exp'; !DIVIDE; %Val] 
            >> fun args -> ValueOf args.[0] / ValueOf args.[2]
            
            [%Val]
            >> fun args -> ValueOf args.[0]
        ]

        Val => [
            [!LPAR; %Exp; !RPAR]
            >> fun args -> ValueOf args.[1]

            [!INT]
            >> fun args -> ValueOf args.[0]

            [!FLOAT]
            >> fun args -> ValueOf args.[0]
            
        ]
    ]
    |> SLR // define parser type.
```
The Productions keyword above is simply the type the parser takes.
we have the form
    Productions (production list)
where a production have the form
    nonterminal => (rule >> action) list
the rule has the from 
   [%nonterminal; !terminal]
and the action is some function taking an array of arguments. disclaimers, it will panic if trying to collect an index bigger than the number of arguments specificed by the rule (they are zero based). the prefix % defined the recursive pattern of a nonterminal, and the prefix ! define the expectation of the instance of the token type.
As mentioned above, we here use the 3 functioned defined above. if we instead of directly computing the value but definde a syntax tree structure it could be expressed as

```Fsharp
open Productions
open Parser
open Position

// This is the lexing pattern for the compiler
type TokenType = 
    | INT  | FLOAT // | ID    | STR 
    | PLUS | MINUS | TIMES | DIVIDE 
    | LPAR | RPAR  | NOISE  // | LSQR | RSQR | LBRA | RBRA
    
type Expression = 
    | F64 of float 
    | I32 of int
    | Add of Expression * Expression * Position
    | Sub of Expression * Expression * Position
    | Mul of Expression * Expression * Position
    | Div of Expression * Expression * Position

// this is the parsing pattern for the compiler
type e = Exp | Exp' | Val

// OBS! if for some reason you havn't implemented all Tokens into the productions, it will course an error
let parser =
    Productions [
        Exp => [
            [%Exp; !PLUS; %Exp'] 
            >> fun args -> Add(args.[0], ValueOf args.[2], PosOf args.[1])

            [%Exp; !MINUS; %Exp'] 
            >> fun args -> Sub(args.[0], ValueOf args.[2], PosOf args.[1])

            [%Exp']
            >> fun args -> ValueOf args.[0]
        ]

        Exp' => [
            [%Exp'; !TIMES; %Val] 
            >> fun args -> Mul(args.[0], ValueOf args.[2], PosOf args.[1])

            [%Exp'; !DIVIDE; %Val] 
            >> fun args -> Div(args.[0], ValueOf args.[2], PosOf args.[1])
            
            [%Val]
            >> fun args -> ValueOf args.[0]
        ]

        Val => [
            [!LPAR; %Exp; !RPAR]
            >> fun args -> ValueOf args.[1]

            [!INT]
            >> fun args -> I32 (ValueOf args.[0])

            [!FLOAT]
            >> fun args -> F64 (ValueOf args.[0])
            
        ]
    ]
    |> SLR
```

To run the parser over the token sequence we simply use the Run function. The full pipeline are
```Fsharp
    let content = FromString " 1 + 2 - 2 * 3"
    Tokens lexer content                               // return a sequence of tokens from the language of the 'lexer' as put
    |> Seq.filter (fun token -> TypeOf token <> NOISE) // user defined filter of tokens
    |> Run parser                                      // parse token sequence
    |> printfn "result: %A"
```




